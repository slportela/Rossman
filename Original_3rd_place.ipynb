{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "\n",
    "def csv2dicts(csvfile):\n",
    "    data = []\n",
    "    keys = []\n",
    "    for row_index, row in enumerate(csvfile):\n",
    "        if row_index == 0:\n",
    "            keys = row\n",
    "            print(row)\n",
    "            continue\n",
    "        # if row_index % 10000 == 0:\n",
    "        #     print(row_index)\n",
    "        data.append({key: value for key, value in zip(keys, row)})\n",
    "    return data\n",
    "\n",
    "\n",
    "def set_nan_as_string(data, replace_str='0'):\n",
    "    for i, x in enumerate(data):\n",
    "        for key, value in x.items():\n",
    "            if value == '':\n",
    "                x[key] = replace_str\n",
    "        data[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_filename = \"dataset/rossmann/test.csv\"\n",
    "train_data_filename = \"dataset/rossmann/train.csv\"\n",
    "store_data_filename = \"dataset/rossmann/store.csv\"\n",
    "store_states_filename = 'dataset/rossmann/store_states.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
      "[{'Store': '1115', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1114', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1113', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}]\n"
     ]
    }
   ],
   "source": [
    "csvfile = open(train_data_filename)\n",
    "train_data = csv.reader(csvfile, delimiter=',')\n",
    "train_data = csv2dicts(train_data)\n",
    "train_data = train_data[::-1]\n",
    "\n",
    "print(train_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
      "[{'Id': '41088', 'Store': '1115', 'DayOfWeek': '6', 'Date': '2015-08-01', 'Open': '1', 'Promo': '0', 'StateHoliday': '0', 'SchoolHoliday': '1'}, {'Id': '41087', 'Store': '1114', 'DayOfWeek': '6', 'Date': '2015-08-01', 'Open': '1', 'Promo': '0', 'StateHoliday': '0', 'SchoolHoliday': '0'}, {'Id': '41086', 'Store': '1113', 'DayOfWeek': '6', 'Date': '2015-08-01', 'Open': '1', 'Promo': '0', 'StateHoliday': '0', 'SchoolHoliday': '0'}]\n"
     ]
    }
   ],
   "source": [
    "csvfile_test = open(test_data_filename)\n",
    "test_data = csv.reader(csvfile_test, delimiter=',')\n",
    "test_data = csv2dicts(test_data)\n",
    "test_data = test_data[::-1]\n",
    "\n",
    "print(test_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
      "['Store', 'State']\n",
      "[{'Store': '1', 'StoreType': 'c', 'Assortment': 'a', 'CompetitionDistance': '1270', 'CompetitionOpenSinceMonth': '9', 'CompetitionOpenSinceYear': '2008', 'Promo2': '0', 'Promo2SinceWeek': '0', 'Promo2SinceYear': '0', 'PromoInterval': '0', 'State': 'HE'}, {'Store': '2', 'StoreType': 'a', 'Assortment': 'a', 'CompetitionDistance': '570', 'CompetitionOpenSinceMonth': '11', 'CompetitionOpenSinceYear': '2007', 'Promo2': '1', 'Promo2SinceWeek': '13', 'Promo2SinceYear': '2010', 'PromoInterval': 'Jan,Apr,Jul,Oct', 'State': 'TH'}]\n"
     ]
    }
   ],
   "source": [
    "csvfile = open(store_data_filename)\n",
    "csvfile2 = open(store_states_filename)\n",
    "store_data = csv.reader(csvfile, delimiter=',')\n",
    "store_states_data = csv.reader(csvfile2, delimiter=',')\n",
    "\n",
    "store_data = csv2dicts(store_data)\n",
    "store_states_data = csv2dicts(store_states_data)\n",
    "set_nan_as_string(store_data)\n",
    "for index, val in enumerate(store_data):\n",
    "    state = store_states_data[index]\n",
    "    val['State'] = state['State']\n",
    "    store_data[index] = val\n",
    "\n",
    "print(store_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def feature_list(record):\n",
    "    dt = datetime.strptime(record['Date'], '%Y-%m-%d')\n",
    "    store_index = int(record['Store'])\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    day_of_week = int(record['DayOfWeek'])\n",
    "    try:\n",
    "        store_open = int(record['Open'])\n",
    "    except:\n",
    "        store_open = 1\n",
    "\n",
    "    promo = int(record['Promo'])\n",
    "\n",
    "    return [store_open,\n",
    "            store_index,\n",
    "            day_of_week,\n",
    "            promo,\n",
    "            year,\n",
    "            month,\n",
    "            day,\n",
    "            store_data[store_index - 1]['State']\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train datapoints:  844338\n",
      "46 41551\n"
     ]
    }
   ],
   "source": [
    "train_data_X = []\n",
    "train_data_y = []\n",
    "\n",
    "for record in train_data:\n",
    "    if record['Sales'] != '0' and record['Open'] != '':\n",
    "        fl = feature_list(record)\n",
    "        train_data_X.append(fl)\n",
    "        train_data_y.append(int(record['Sales']))\n",
    "print(\"Number of train datapoints: \", len(train_data_y))\n",
    "print(min(train_data_y), max(train_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1097, 2, 0, 2013, 1, 1, 'RP'],\n",
       " [1, 948, 2, 0, 2013, 1, 1, 'BW'],\n",
       " [1, 769, 2, 0, 2013, 1, 1, 'NW'],\n",
       " [1, 733, 2, 0, 2013, 1, 1, 'NW'],\n",
       " [1, 682, 2, 0, 2013, 1, 1, 'BE'],\n",
       " [1, 676, 2, 0, 2013, 1, 1, 'HE'],\n",
       " [1, 562, 2, 0, 2013, 1, 1, 'HB,NI'],\n",
       " [1, 530, 2, 0, 2013, 1, 1, 'SH'],\n",
       " [1, 512, 2, 0, 2013, 1, 1, 'BY'],\n",
       " [1, 494, 2, 0, 2013, 1, 1, 'BE']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = []\n",
    "for record in test_data:\n",
    "    fl = feature_list(record)\n",
    "    test_data_X.append(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1115, 6, 0, 2015, 8, 1, 'HE'],\n",
       " [1, 1114, 6, 0, 2015, 8, 1, 'HH'],\n",
       " [1, 1113, 6, 0, 2015, 8, 1, 'SH'],\n",
       " [1, 1112, 6, 0, 2015, 8, 1, 'NW'],\n",
       " [1, 1111, 6, 0, 2015, 8, 1, 'NW'],\n",
       " [1, 1109, 6, 0, 2015, 8, 1, 'BY'],\n",
       " [1, 1107, 6, 0, 2015, 8, 1, 'BY'],\n",
       " [1, 1106, 6, 0, 2015, 8, 1, 'SH'],\n",
       " [1, 1105, 6, 0, 2015, 8, 1, 'NW'],\n",
       " [1, 1104, 6, 0, 2015, 8, 1, 'BY']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844338, 8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_X = np.array(train_data_X)\n",
    "train_data_X = np.array(train_data_X)\n",
    "full_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41088, 8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_X = np.array(test_data_X)\n",
    "test_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1', ..., '1', '1', '1'], dtype='<U21')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "les = []\n",
    "for i in range(train_data_X.shape[1]):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(full_X[:, i])\n",
    "    les.append(le)\n",
    "    train_data_X[:, i] = le.transform(train_data_X[:, i])\n",
    "    if i > 0:\n",
    "        test_data_X[:, i] = le.transform(test_data_X[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844338, 8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X = train_data_X.astype(int)\n",
    "train_data_y = np.array(train_data_y)\n",
    "# les y  (train_data_X, train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 109   1   0   0   0   0   7] 5961\n"
     ]
    }
   ],
   "source": [
    "print(train_data_X[0], train_data_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file\n",
    "\n",
    "(X, y) = train_data_X, train_data_y\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759904, 8) (84434, 8)\n"
     ]
    }
   ],
   "source": [
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    np.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "if embeddings_as_input:\n",
    "    print(\"Using learned embeddings as input\")\n",
    "    X = embed_features(X, saved_embeddings_fname)\n",
    "\n",
    "if one_hot_as_input:\n",
    "    print(\"Using one-hot encoding as input\")\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = np.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]\n",
    "\n",
    "\n",
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN_with_EntityEmbedding...\n",
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.0143 - val_loss: 0.0114\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0094 - val_loss: 0.0112\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0085 - val_loss: 0.0111\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0077 - val_loss: 0.0096\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0075 - val_loss: 0.0102\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0073 - val_loss: 0.0092\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0072 - val_loss: 0.0095\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0069 - val_loss: 0.0092\n",
      "Result on validation data:  0.09966181524657178\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0144 - val_loss: 0.0113\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0095 - val_loss: 0.0109\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0080 - val_loss: 0.0106\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0077 - val_loss: 0.0097\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0074 - val_loss: 0.0096\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0072 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0071 - val_loss: 0.0100\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Result on validation data:  0.09956620769171172\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0141 - val_loss: 0.0109\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0091 - val_loss: 0.0105\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0083 - val_loss: 0.0103\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0079 - val_loss: 0.0099\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0076 - val_loss: 0.0095\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0074 - val_loss: 0.0093\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0072 - val_loss: 0.0093\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0070 - val_loss: 0.0093\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0069 - val_loss: 0.0095\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Result on validation data:  0.10021336133841634\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0143 - val_loss: 0.0110\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0095 - val_loss: 0.0113\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0088 - val_loss: 0.0105\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0081 - val_loss: 0.0110\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0077 - val_loss: 0.0101\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0074 - val_loss: 0.0106\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0072 - val_loss: 0.0104\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0070 - val_loss: 0.0101\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0069 - val_loss: 0.0107\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Result on validation data:  0.11053671209919079\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0139 - val_loss: 0.0123\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0094 - val_loss: 0.0112\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0085 - val_loss: 0.0121\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0076 - val_loss: 0.0100\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0074 - val_loss: 0.0102\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0072 - val_loss: 0.0096\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0071 - val_loss: 0.0100\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0069 - val_loss: 0.0109\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0068 - val_loss: 0.0105\n",
      "Result on validation data:  0.11880744088175825\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "print(\"Fitting NN_with_EntityEmbedding...\")\n",
    "for i in range(5):\n",
    "    models.append(NN_with_EntityEmbedding(X_train, y_train, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41088, 7)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_X[:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_norm = models[0].model.predict(np.hsplit(test_data_X[:, 1:], 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = np.exp(predictions_norm * models[0].max_log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test[(test_data_X[:, 0] == '0')] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_csv = pd.read_csv('dataset/rossmann/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv['Sales'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv.to_csv(f'submision_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6987.087402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>23844.628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6288.915527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7890.188477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3279.626221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id         Sales\n",
       "0   1   6987.087402\n",
       "1   2  23844.628906\n",
       "2   3   6288.915527\n",
       "3   4   7890.188477\n",
       "4   5   3279.626221"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.0635817444220401\n",
      "Validation error...\n",
      "0.09371728533670369\n"
     ]
    }
   ],
   "source": [
    "# print(\"Fitting NN...\")\n",
    "# for i in range(5):\n",
    "#     models.append(NN(X_train, y_train, X_val, y_val))\n",
    "\n",
    "# print(\"Fitting RF...\")\n",
    "# models.append(RF(X_train, y_train, X_val, y_val))\n",
    "\n",
    "# print(\"Fitting KNN...\")\n",
    "# models.append(KNN(X_train, y_train, X_val, y_val))\n",
    "\n",
    "# print(\"Fitting XGBoost...\")\n",
    "# models.append(XGBoost(X_train, y_train, X_val, y_val))\n",
    "\n",
    "\n",
    "if save_embeddings:\n",
    "    model = models[0].model\n",
    "    store_embedding = model.get_layer('store_embedding').get_weights()[0]\n",
    "    dow_embedding = model.get_layer('dow_embedding').get_weights()[0]\n",
    "    year_embedding = model.get_layer('year_embedding').get_weights()[0]\n",
    "    month_embedding = model.get_layer('month_embedding').get_weights()[0]\n",
    "    day_embedding = model.get_layer('day_embedding').get_weights()[0]\n",
    "    german_states_embedding = model.get_layer('state_embedding').get_weights()[0]\n",
    "    with open(saved_embeddings_fname, 'wb') as f:\n",
    "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
    "                     month_embedding, day_embedding, german_states_embedding], f, -1)\n",
    "\n",
    "\n",
    "def evaluate_models(models, X, y):\n",
    "    assert(min(y) > 0)\n",
    "    guessed_sales = numpy.array([model.guess(X) for model in models])\n",
    "    mean_sales = guessed_sales.mean(axis=0)\n",
    "    relative_err = numpy.absolute((y - mean_sales) / y)\n",
    "    result = numpy.sum(relative_err) / len(y)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-feecb69a18cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
